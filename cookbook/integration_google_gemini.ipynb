{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KI9E73QVVAdU"
      },
      "source": [
        "<!-- NOTEBOOK_METADATA source: \"⚠️ Jupyter Notebook\" title: \"Observability for Google Gemini Models with Langfuse Integration\" sidebarTitle: \"Google Gemini\" logo: \"/images/integrations/google_gemini_icon.svg\" description: \"Learn how to integrate Langfuse with the Google GenAI SDK for comprehensive tracing and debugging of your AI conversations.\" category: \"Integrations\" -->\n",
        "\n",
        "# Trace Google Gemini Models in Langfuse\n",
        "\n",
        "This notebook shows how to trace and observe Google Gemini models with Langfuse and the Google GenAI SDK.\n",
        "\n",
        "> **What is Google Gemini?** [Google Gemini](https://ai.google.dev/gemini-api/docs/libraries) is Google’s family of multimodal generative models (text, images, audio, video, code) available through the Gemini API and Vertex AI, with tiers like Flash and Pro for different speed/quality needs.\n",
        "\n",
        "> **What is the Google GenAI SDK?** The [Google GenAI SDK](https://cloud.google.com/vertex-ai/generative-ai/docs/sdks/overview) is a unified client library (Python/JavaScript) that simplifies calling Gemini—handling auth (API key or ADC), streaming, tool/function calling, and safety—so you can integrate models in a few lines.\n",
        "\n",
        "> **What is Langfuse?** [Langfuse](https://langfuse.com) is an open source platform for LLM observability and monitoring. It helps you trace and monitor your AI applications by capturing metadata, prompt details, token usage, latency, and more.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFT1l6AhVAdV"
      },
      "source": [
        "<!-- STEPS_START -->\n",
        "## Step 1: Install Dependencies\n",
        "\n",
        "Before you begin, install the necessary packages in your Python environment:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLq6qvw2VAdV",
        "outputId": "704b6db0-e912-4abe-c654-bfb36190b953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.45.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: langfuse in /usr/local/lib/python3.12/dist-packages (3.8.1)\n",
            "Requirement already satisfied: openinference-instrumentation-google-genai in /usr/local/lib/python3.12/dist-packages (0.1.8)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.11.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.11.10)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.32.4)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.15.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langfuse) (2.2.1)\n",
            "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from langfuse) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from langfuse) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from langfuse) (1.37.0)\n",
            "Requirement already satisfied: packaging<26.0,>=23.2 in /usr/local/lib/python3.12/dist-packages (from langfuse) (25.0)\n",
            "Requirement already satisfied: wrapt<2.0,>=1.14 in /usr/local/lib/python3.12/dist-packages (from langfuse) (1.17.3)\n",
            "Requirement already satisfied: openinference-instrumentation>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from openinference-instrumentation-google-genai) (0.1.41)\n",
            "Requirement already satisfied: openinference-semantic-conventions in /usr/local/lib/python3.12/dist-packages (from openinference-instrumentation-google-genai) (0.1.24)\n",
            "Requirement already satisfied: opentelemetry-instrumentation in /usr/local/lib/python3.12/dist-packages (from openinference-instrumentation-google-genai) (0.58b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions in /usr/local/lib/python3.12/dist-packages (from openinference-instrumentation-google-genai) (0.58b0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (1.71.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (1.37.0)\n",
            "Requirement already satisfied: protobuf<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-proto==1.37.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (5.29.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->langfuse) (3.23.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install google-genai openai langfuse openinference-instrumentation-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz9nkPkHVAdW"
      },
      "source": [
        "## Step 2: Configure Langfuse SDK\n",
        "\n",
        "Next, set up your Langfuse API keys. You can get these keys by signing up for a free [Langfuse Cloud](https://cloud.langfuse.com/) account or by [self-hosting Langfuse](https://langfuse.com/self-hosting). These environment variables are essential for the Langfuse client to authenticate and send data to your Langfuse project.\n",
        "\n",
        "Also set your Google Vertex API credentials which uses Application Default Credentials (ADC) from a service account key file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PhheOIaSVAdW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get keys for your project from the project settings page\n",
        "# https://cloud.langfuse.com\n",
        "LANGFUSE_SECRET_KEY =\n",
        "LANGFUSE_PUBLIC_KEY =\n",
        "# LANGFUSE_BASE_URL = \"https://cloud.langfuse.com\" # 🇪🇺 EU region\n",
        "LANGFUSE_BASE_URL = \"https://us.cloud.langfuse.com\" # 🇺🇸 US region\n",
        "\n",
        "# Your openai key\n",
        "# We use OpenAI for this demo, could easily change to other models\n",
        "os.environ[\"GEMINI_API_KEY\"] ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vUwHWeIVAdW"
      },
      "source": [
        "With the environment variables set, we can now initialize the Langfuse client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06LbPigOVAdW",
        "outputId": "556fa83d-6d4f-490a-aeb0-cc171d871561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langfuse:Authentication error: Langfuse client initialized without public_key. Client will be disabled. Provide a public_key parameter or set LANGFUSE_PUBLIC_KEY environment variable. \n"
          ]
        }
      ],
      "source": [
        "from langfuse import get_client\n",
        "\n",
        "# Initialise Langfuse client and verify connectivity\n",
        "langfuse = get_client()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuqJ1aC3VAdW"
      },
      "source": [
        "## Step 3: OpenTelemetry Instrumentation\n",
        "\n",
        "Use the [`GoogleGenAIInstrumentor`](https://github.com/Arize-ai/openinference/tree/main/python/instrumentation/openinference-instrumentation-google-genai) library to wrap [Google GenAI SDK](https://ai.google.dev/gemini-api/docs/libraries) calls and send OpenTelemetry spans to Langfuse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eIPAphfxVAdW"
      },
      "outputs": [],
      "source": [
        "from openinference.instrumentation.google_genai import GoogleGenAIInstrumentor\n",
        "\n",
        "GoogleGenAIInstrumentor().instrument()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k9CJhShVAdX"
      },
      "source": [
        "## Step 4: Run an Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5dgQhScVAdX",
        "outputId": "3f545483-59f3-4a12-cb0a-baed66b64083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Langfuse** is an **open-source LLM observability and evaluation platform**.\n",
            "\n",
            "In simpler terms, it's a tool that helps developers building applications powered by Large Language Models (LLMs) to **monitor, debug, evaluate, and improve** their applications throughout their lifecycle, from development to production.\n",
            "\n",
            "Developing with LLMs is often iterative and complex. Langfuse addresses this by providing the infrastructure to understand what your LLM app is doing, how well it's performing, and how to make it better.\n",
            "\n",
            "Here's a breakdown of its core functionalities:\n",
            "\n",
            "1.  **Observability & Tracing:**\n",
            "    *   **What it does:** It allows you to \"see\" every step of an LLM call or an entire chain of calls. This includes the initial prompt, the LLM's response, any intermediate steps (like RAG retrievals, tool calls, function calls), and metadata (latency, token counts, costs).\n",
            "    *   **Why it's useful:**\n",
            "        *   **Debugging:** Pinpoint exactly where an issue (e.g., wrong answer, high latency) occurred within a complex chain.\n",
            "        *   **Understanding behavior:** See how your prompts are being interpreted and how different components of your app interact.\n",
            "        *   **Cost & Performance Analysis:** Track token usage and latency for individual calls and entire applications.\n",
            "\n",
            "2.  **Evaluation & Feedback:**\n",
            "    *   **What it does:** Langfuse provides tools to systematically evaluate the quality of your LLM's outputs.\n",
            "        *   **Human Evaluation:** Allows testers or users to provide scores, ratings (e.g., like/dislike), and comments on specific traces or outputs.\n",
            "        *   **LLM-assisted Evaluation:** You can use another LLM to automatically evaluate the quality of your application's output based on predefined criteria (e.g., \"is the answer relevant?\", \"is it concise?\").\n",
            "        *   **Automated Metrics:** Tracks built-in metrics like latency, cost, and token usage.\n",
            "    *   **Why it's useful:**\n",
            "        *   **Measure Progress:** Quantify improvements from prompt changes, model updates, or RAG enhancements.\n",
            "        *   **A/B Testing:** Compare different versions of your prompts or models.\n",
            "        *   **Data Collection:** Gather high-quality data for potential fine-tuning or further analysis.\n",
            "        *   **Feedback Loops:** Integrate user feedback directly into your development process.\n",
            "\n",
            "3.  **Prompt Management (part of observability):**\n",
            "    *   While not a dedicated prompt management system, Langfuse helps you track which prompt templates were used for which trace, aiding in version control and understanding how prompt changes impact performance.\n",
            "\n",
            "**Why is Langfuse important for LLM development?**\n",
            "\n",
            "*   **Complexity:** LLM applications, especially those involving multiple steps (RAG, agents, tool use), can be hard to reason about. Langfuse provides visibility.\n",
            "*   **Iterative Nature:** Developing good LLM apps requires constant experimentation and refinement. Langfuse makes it easier to measure the impact of these iterations.\n",
            "*   **Production Readiness:** Moving an LLM app from a prototype to a reliable production system requires robust monitoring and evaluation capabilities.\n",
            "\n",
            "**Key Features/Benefits at a glance:**\n",
            "\n",
            "*   **Open-Source:** You can host it yourself, inspect the code, and contribute to the community.\n",
            "*   **Integrations:** Natively integrates with popular LLM frameworks like LangChain and LlamaIndex, as well as various LLM providers.\n",
            "*   **End-to-End:** Covers the full lifecycle from development to production monitoring.\n",
            "*   **UI Dashboard:** Provides a user-friendly interface to visualize traces, evaluations, and metrics.\n",
            "\n",
            "In essence, Langfuse is like a \"control tower\" for your LLM applications, giving you the visibility and tools needed to build, test, and deploy them with confidence.\n"
          ]
        }
      ],
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=\"What is Langfuse?\",\n",
        ")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eds93KOTVAdX",
        "outputId": "4d6486f3-e8c5-49a1-e916-732b8c6eafca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Langfuse is an open-source platform designed for observability and evaluation of Large Language Model (LLM) applications.**\n",
            "\n",
            "In simpler terms, it's a tool that helps developers and teams:\n",
            "\n",
            "1.  **See what's happening inside their LLM-powered applications** (observability).\n",
            "2.  **Measure and improve the performance and quality of those applications** (evaluation).\n",
            "\n",
            "### Why is Langfuse Needed?\n",
            "\n",
            "Building with LLMs can be complex. Unlike traditional software, LLMs are non-deterministic, their outputs can be unpredictable, and it's often hard to debug why a certain prompt led to a bad response or how a chain of LLM calls is actually performing. Langfuse addresses these challenges by providing:\n",
            "\n",
            "*   **Lack of Visibility:** It's hard to know what inputs, outputs, and intermediate steps occur during an LLM interaction.\n",
            "*   **Difficulty in Debugging:** When an LLM application breaks or gives poor responses, tracing the root cause is challenging.\n",
            "*   **Challenges in Evaluation:** How do you objectively measure if a new prompt, model, or RAG strategy is actually better?\n",
            "*   **Cost Management:** LLM calls incur costs (per token). Understanding usage is crucial.\n",
            "\n",
            "### Key Features and What Langfuse Does:\n",
            "\n",
            "1.  **Tracing & Logging:**\n",
            "    *   **End-to-end traces:** It records every step of an LLM interaction, including user input, prompt templates, model calls (input/output, model used, latency, token count, cost), RAG retrievals, tool usage, and final application output. This provides a \"call stack\" for your LLM app.\n",
            "    *   **Detailed logs:** Every prompt, every response, and all metadata associated with it.\n",
            "\n",
            "2.  **Monitoring & Analytics:**\n",
            "    *   **Real-time dashboards:** Visualize key metrics like latency, token usage, cost, error rates, and user feedback over time.\n",
            "    *   **Filtering and Search:** Easily find specific traces based on user IDs, session IDs, prompt parameters, and more.\n",
            "\n",
            "3.  **Evaluation & Feedback:**\n",
            "    *   **Human Annotation:** Allows developers or users to provide feedback (e.g., \"good,\" \"bad,\" \"relevant\") on specific traces or outputs, which is critical for fine-tuning and improvement.\n",
            "    *   **Programmatic Evaluation:** Integrate with evaluation metrics to automatically score responses (e.g., for factual accuracy, coherence).\n",
            "    *   **Batch Evaluation:** Run tests on datasets to compare different prompt versions, models, or RAG strategies.\n",
            "\n",
            "4.  **Prompt Engineering & A/B Testing:**\n",
            "    *   Compare the performance of different prompts or models side-by-side using real-world data or evaluation sets.\n",
            "    *   Track how changes to your prompts affect performance metrics.\n",
            "\n",
            "5.  **Integrations:**\n",
            "    *   Provides SDKs for popular LLM frameworks like **LangChain** and **LlamaIndex**, as well as direct integrations with LLM providers like **OpenAI**, **Anthropic**, **Mistral**, etc.\n",
            "    *   Offers Python and TypeScript/JavaScript SDKs.\n",
            "\n",
            "### Who Uses Langfuse?\n",
            "\n",
            "*   **LLM Developers & AI Engineers:** To debug, monitor, and optimize their GenAI applications.\n",
            "*   **Product Managers:** To understand application performance, user engagement, and areas for improvement.\n",
            "*   **Data Scientists:** To evaluate model performance, test new strategies, and gather feedback for iterative development.\n",
            "\n",
            "In essence, Langfuse acts as the \"mission control\" for your LLM applications, giving you the transparency and tools needed to build robust, reliable, and high-performing AI products.\n"
          ]
        }
      ],
      "source": [
        "# Streaming Example\n",
        "for chunk in client.models.generate_content_stream(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=\"What is Langfuse?\",\n",
        "):\n",
        "    print(chunk.text, end=\"\", flush=True)\n",
        "print()  # newline after streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDsElsI-VAdX"
      },
      "source": [
        "### View Traces in Langfuse\n",
        "\n",
        "After executing the application, navigate to your Langfuse Trace Table. You will find detailed traces of the application's execution, providing insights into the agent conversations, LLM calls, inputs, outputs, and performance metrics.\n",
        "\n",
        "![Langfuse Trace](https://langfuse.com/images/cookbook/integration_gemini/gemini-trace.png)\n",
        "\n",
        "[View trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/9f2f0fe0228fd81a9fe75882934b384a?timestamp=2025-08-01T13%3A22%3A00.147Z&display=details&observation=b7a63ca7e1d083bc)\n",
        "\n",
        "<!-- STEPS_END -->\n",
        "\n",
        "<!-- MARKDOWN_COMPONENT name: \"LearnMore\" path: \"@/components-mdx/integration-learn-more.mdx\" -->"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}